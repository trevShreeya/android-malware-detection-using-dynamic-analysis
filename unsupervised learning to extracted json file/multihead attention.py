#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Aug 16 14:54:22 2024

@author: hpc
"""
#%%
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, Flatten, MultiHeadAttention, LayerNormalization, Dropout, concatenate
from tensorflow.keras.utils import to_categorical

# Load the dataset
data = pd.read_csv('/home/hpc/Desktop/finalfeatures.csv')

# Display the first few rows of the dataset
data.head()

#%%
# Encode the 'Category' column
label_encoder = LabelEncoder()
data['Category'] = label_encoder.fit_transform(data['Category'])

# Separate features and target
X = data.drop('Category', axis=1).values
y = data['Category'].values

# One-hot encode the target labels
y = to_categorical(y)

# Standardize the feature values
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Reshape X for LSTM (samples, timesteps, features) assuming 1 timestep
X = X.reshape(X.shape[0], 1, X.shape[1])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#%%
def multi_head_attention(inputs, num_heads):
    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)
    attention_output = LayerNormalization()(attention_output + inputs)
    return attention_output

#%%
# Input layer
input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))

# LSTM Layer
lstm_layer = LSTM(128, return_sequences=True)(input_layer)

# Multi-Head Attention Layer
attention_layer = multi_head_attention(lstm_layer, num_heads=4)

# CNN Layer with a smaller kernel size
cnn_layer = Conv1D(filters=64, kernel_size=1, activation='relu')(attention_layer)
cnn_layer = Flatten()(cnn_layer)

# Fully Connected Layers
dense_layer = Dense(128, activation='relu')(cnn_layer)
dense_layer = Dropout(0.5)(dense_layer)
output_layer = Dense(y_train.shape[1], activation='softmax')(dense_layer)

# Create the model
model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy * 100:.2f}%')
#%%
import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
