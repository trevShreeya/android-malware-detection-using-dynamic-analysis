#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Aug 16 16:11:34 2024

@author: hpc
"""
#%%
print("for random 14 classes")

import pandas as pd

# Load the dataset
data = pd.read_csv('/home/hpc/Desktop/finalfeatures.csv')

#%%
from sklearn.preprocessing import LabelEncoder

# Encode the 'Category' column
label_encoder = LabelEncoder()
data['Category'] = label_encoder.fit_transform(data['Category'])

#%%
# Separate features and 
X = data.drop('Category', axis=1).values
y = data['Category'].values
#%%

from sklearn.preprocessing import StandardScaler

# Standardize the feature values
scaler = StandardScaler()
X = scaler.fit_transform(X)
#%%
from sklearn.model_selection import train_test_split

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#%%
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Conv1D, LSTM, Flatten, MultiHeadAttention, LayerNormalization, Dropout

def multi_head_attention(inputs, num_heads):
    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)
    attention_output = LayerNormalization()(attention_output + inputs)
    return attention_output

def build_model():
    input_layer = Input(shape=(X_train.shape[1], 1))  # Adjust shape if needed
    cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input_layer)
    lstm_layer = LSTM(128, return_sequences=True)(cnn_layer)
    attention_layer = multi_head_attention(lstm_layer, num_heads=4)
    flattened_layer = Flatten()(attention_layer)
    dense_layer = Dense(128, activation='relu')(flattened_layer)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(label_encoder.classes_), activation='softmax')(dense_layer)
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

model = build_model()
#%%
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))
#%%
# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy * 100:.2f}%')
#%%
# Predict with the model
predictions = model.predict(X_test)
predicted_classes = tf.argmax(predictions, axis=1).numpy()

# Convert numerical predictions back to original class labels
predicted_labels = label_encoder.inverse_transform(predicted_classes)
