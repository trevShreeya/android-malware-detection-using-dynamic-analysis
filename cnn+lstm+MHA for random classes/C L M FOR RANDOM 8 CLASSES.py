#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Aug 17 15:15:52 2024

@author: hpc
"""

print("for random 8 classes")
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Conv1D, LSTM, Flatten, MultiHeadAttention, LayerNormalization, Dropout

# Load the dataset
data = pd.read_csv('/home/hpc/Desktop/finalfeatures.csv')

# Encode the 'Category' column
label_encoder = LabelEncoder()
data['Category'] = label_encoder.fit_transform(data['Category'])

# Get class names and randomly select 8 classes
class_names = label_encoder.classes_
target_classes = np.random.choice(class_names, size=8, replace=False)

# Map target classes to their respective indices
target_indices = [list(class_names).index(c) for c in target_classes]

# Filter the dataset for only these 8 classes
data_filtered = data[data['Category'].isin(target_indices)]

# Ensure that filtered labels are within [0, 1]
data_filtered['Category'] = LabelEncoder().fit_transform(data_filtered['Category'])

# Update the labels and features
X = data_filtered.drop('Category', axis=1).values
y = data_filtered['Category'].values

# Standardize the feature values
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Reshape data for CNN input
X = X[..., np.newaxis]  # Add channel dimension

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model definition
def multi_head_attention(inputs, num_heads):
    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)
    attention_output = LayerNormalization()(attention_output + inputs)
    return attention_output

def build_model():
    input_layer = Input(shape=(X_train.shape[1], 1))  # Adjust shape if needed
    cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input_layer)
    lstm_layer = LSTM(128, return_sequences=True)(cnn_layer)
    attention_layer = multi_head_attention(lstm_layer, num_heads=4)
    flattened_layer = Flatten()(attention_layer)
    dense_layer = Dense(128, activation='relu')(flattened_layer)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(8, activation='softmax')(dense_layer)  # Output layer for 8 classes
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

model = build_model()

# Train the model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy * 100:.2f}%')

# Predict with the model
predictions = model.predict(X_test)
predicted_classes = tf.argmax(predictions, axis=1).numpy()

# Convert numerical predictions back to original class labels
predicted_labels = label_encoder.inverse_transform(predicted_classes)

# Print the selected classes and predicted labels
print(f'Selected classes: {target_classes}')
print(predicted_labels)

