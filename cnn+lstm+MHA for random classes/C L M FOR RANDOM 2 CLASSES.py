#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Aug 17 12:16:33 2024

@author: hpc
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Conv1D, LSTM, Flatten, MultiHeadAttention, LayerNormalization, Dropout

# Load the dataset
data = pd.read_csv('/home/hpc/Desktop/finalfeatures.csv')

# Encode the 'Category' column
label_encoder = LabelEncoder()
data['Category'] = label_encoder.fit_transform(data['Category'])

# Get class names and randomly select two classes
class_names = label_encoder.classes_
target_classes = np.random.choice(class_names, size=2, replace=False)

# Map target classes to their respective indices
target_indices = [list(class_names).index(c) for c in target_classes]

# Filter the dataset for only these two classes
data_filtered = data[data['Category'].isin(target_indices)]

# Ensure that filtered labels are within [0, 1]
data_filtered['Category'] = LabelEncoder().fit_transform(data_filtered['Category'])

# Update the labels and features
X = data_filtered.drop('Category', axis=1).values
y = data_filtered['Category'].values

# Standardize the feature values
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Reshape data for CNN input
X = X[..., np.newaxis]  # Add channel dimension

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model definition
def multi_head_attention(inputs, num_heads):
    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)
    attention_output = LayerNormalization()(attention_output + inputs)
    return attention_output

def build_model():
    input_layer = Input(shape=(X_train.shape[1], 1))  # Adjust shape if needed
    cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input_layer)
    lstm_layer = LSTM(128, return_sequences=True)(cnn_layer)
    attention_layer = multi_head_attention(lstm_layer, num_heads=4)
    flattened_layer = Flatten()(attention_layer)
    dense_layer = Dense(128, activation='relu')(flattened_layer)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(2, activation='softmax')(dense_layer)  # Output layer for 2 classes
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

model = build_model()

# Train the model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy * 100:.2f}%')

# Predict with the model
predictions = model.predict(X_test)
predicted_classes = tf.argmax(predictions, axis=1).numpy()

# Convert numerical predictions back to original class labels
predicted_labels = label_encoder.inverse_transform(predicted_classes)

# Print the selected classes and predicted labels
print(f'Selected classes: {target_classes}')
print(predicted_labels)

#%%
from sklearn.metrics import confusion_matrix, roc_curve, auc

# Predict class probabilities
y_proba = model.predict(X_test)

# Convert probabilities to binary predictions (for each class, here we assume binary classification for simplicity)
y_pred = np.argmax(y_proba, axis=1)

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:')
print(cm)

# Compute TPR and FPR for each class
tn, fp, fn, tp = cm.ravel()
tpr = tp / (tp + fn)  # True Positive Rate
fpr = fp / (fp + tn)  # False Positive Rate

print(f'True Positive Rate (TPR): {tpr:.2f}')
print(f'False Positive Rate (FPR): {fpr:.2f}')

# Compute ROC curve
fpr_values, tpr_values, _ = roc_curve(y_test, y_proba[:, 1])
roc_auc = auc(fpr_values, tpr_values)

print(f'ROC AUC: {roc_auc:.2f}')
