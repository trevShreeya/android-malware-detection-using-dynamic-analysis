# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
# Load your dataset (replace with your actual data loading code)
data = pd.read_csv('/home/hpc/Desktop/benign or malicious /android_traffic1.csv')
data.head(9)
# Check for missing values
print(data.isnull().sum())
# Drop columns with many missing values
data.drop(['duracion', 'avg_local_pkt_rate', 'avg_remote_pkt_rate'], axis=1, inplace=True)

# Drop 'name' and 'type' columns
data.drop(['name', 'type'], axis=1, inplace=True)
# Remove rows with NaN values
data.dropna(inplace=True)
data.head(9)
# Separate features and label
X = data.drop(['numeric_type'], axis=1) #selects all columns except numeric_type as features (X).
y = data['numeric_type']   
#selects the numeric_type column as the target variable (y),
#which indicates whether software is benign (0) or malicious (1).
# Train-test split (adjust test_size and random_state as needed)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'GBM': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42),
    'Neural Networks': MLPClassifier(random_state=42, max_iter=1000),
    'XGBoost': XGBClassifier(random_state=42),
    'LightGBM': LGBMClassifier(random_state=42),
    'CatBoost': CatBoostClassifier(random_state=42, verbose=0)
}
# Train and evaluate models
results = {}
for name, model in models.items():
    print(f'Training {name}...')
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'classification_report': classification_report(y_test, y_pred),
        'confusion_matrix': confusion_matrix(y_test, y_pred)
    }
    print(f'{name} Accuracy: {accuracy:.4f}')
    print(f'Classification Report for {name}:\n{classification_report(y_test, y_pred)}\n')

# Plot confusion matrices
plt.figure(figsize=(18, 12))
for i, (name, result) in enumerate(results.items()):
    plt.subplot(3, 3, i + 1)
    sns.heatmap(result['confusion_matrix'], annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malware'], yticklabels=['Benign', 'Malware'])
    plt.title(f'Confusion Matrix - {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
plt.tight_layout()
plt.show()

#to improve accuracy of neural networks
#1. Architecture Design and Hyperparameter Tuning
print("for improving accuracy of neural networks")
print("1. Architecture Design and Hyperparameter Tuning")

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report

# Define parameter grid for neural network
param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50)],  # Number of neurons in each hidden layer
    'activation': ['relu', 'tanh'],                    # Activation functions
    'alpha': [0.0001, 0.001, 0.01]                      # L2 regularization parameter
}

# Initialize neural network model
nn = MLPClassifier(random_state=42, max_iter=1000)

# Perform grid search with cross-validation
grid_search = GridSearchCV(nn, param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best CV Score:", grid_search.best_score_)

# Predict on test set and evaluate
y_pred = grid_search.predict(X_test)
print("\nTest Set Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

#2. Feature Scaling
print("2. Feature Scaling")
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming X_train_scaled, X_test_scaled, y_train, y_test are already scaled

# Initialize MLPClassifier with best parameters from tuning
nn = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', alpha=0.0001, random_state=42, max_iter=1000)

# Fit MLPClassifier on scaled training data
nn.fit(X_train_scaled, y_train)

# Predict on test data
y_pred = nn.predict(X_test_scaled)

# Evaluate performance
print("Test Set Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

print("for improving accuracy of logistic regression")
print("1. hyperparameter Tuning")
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report

# Define parameter grid for logistic regression
param_grid = {
    'C': [0.1, 1, 10],            # Regularization parameter
    'penalty': ['l1', 'l2']       # Penalty (l1 for Lasso, l2 for Ridge)
}

# Initialize logistic regression model
log_reg = LogisticRegression(random_state=42, max_iter=1000)

# Perform grid search with cross-validation
grid_search = GridSearchCV(log_reg, param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best CV Score:", grid_search.best_score_)

# Predict on test set and evaluate
y_pred = grid_search.predict(X_test)
print("\nTest Set Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

print("2. Feature Scaling")
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Assuming X_train_scaled, X_test_scaled, y_train, y_test are already scaled

# Initialize logistic regression with best parameters from tuning
log_reg = LogisticRegression(C=1, penalty='l2', random_state=42, max_iter=1000)

# Fit logistic regression on scaled training data
log_reg.fit(X_train_scaled, y_train)

# Predict on test data
y_pred = log_reg.predict(X_test_scaled)

# Evaluate performance
print("Test Set Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

print("TO IMPROVE ACCURACY OF SVM")
print("1.Kernel selection and Hyperparameter Tuning")
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, accuracy_score

# Assuming X_train, X_test, y_train, y_test are already defined

# Create a pipeline with scaling and SVM
pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC(random_state=42))
])

# Define parameters for grid search
param_grid = {
    'svm__C': [0.1, 1, 10],          # Regularization parameter
    'svm__kernel': ['linear', 'rbf'] # Kernel type
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best CV Score:", grid_search.best_score_)

# Predict on test set and evaluate
y_pred = grid_search.predict(X_test)
print("\nTest Set Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

print("2.Feature Scaling")
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Assuming X_train_scaled, X_test_scaled, y_train, y_test are already scaled

# Initialize SVM with best parameters from tuning
svm = SVC(C=1, kernel='rbf', random_state=42)

# Fit SVM on scaled training data
svm.fit(X_train_scaled, y_train)

# Predict on test data
y_pred = svm.predict(X_test_scaled)

# Evaluate performance
print("Test Set Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import matplotlib.pyplot as plt

# Define models and their accuracies
models = ['SVM', 'Logistic Regression', 'Neural Network']
accuracies = [0.7208413001912046, 0.7202039515615042, 0.7788400254939452]

# Plotting the bar chart
plt.figure(figsize=(10, 6))
plt.bar(models, accuracies, color=['blue', 'green', 'orange'])
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Improved Accuracy of Models')
plt.ylim(0.7, 0.925)  # Adjust ylim to better visualize differences if needed
plt.grid(True, axis='y')  # Add gridlines to y-axis
plt.show()


import matplotlib.pyplot as plt
import seaborn as sns

# Accuracy values for each model (including previously mentioned accuracies)
models = ['SVM', 'Logistic Regression', 'Neural Network', 'Random Forest', 'GBM', 'XGBoost', 'LightGBM', 'CatBoost']
accuracies = [0.7208413001912046, 0.7202039515615042, 0.7788400254939452,  # SVM, Logistic Regression, Neural Network
              0.9089, 0.8317, 0.8859, 0.8795, 0.8808]  # Other models

# Plotting the bar chart
plt.figure(figsize=(12, 8))
bars = plt.bar(models, accuracies, color=['blue', 'green', 'orange', 'red', 'purple', 'brown', 'pink', 'gray'])  # Update with colors
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison of Different Models')
plt.ylim(0.7, 0.925)  # Adjust ylim to better visualize differences if needed
plt.grid(True, axis='y')  # Add gridlines to y-axis

# Adding labels to each bar
for bar, accuracy in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,
             f'{accuracy:.4f}', ha='center', va='bottom', fontsize=10)

plt.show()


# Define your example list of dictionaries containing features for each APK
apk_features_list = [
    {
        'tcp_packets': 100,
        'dist_port_tcp': 5,
        'external_ips': 2,
        'volume_bytes': 500000,
        'udp_packets': 50,
        'tcp_urg_packet': 1,
        'source_app_packets': 200,
        'remote_app_packets': 180,
        'source_app_bytes': 100000,
        'remote_app_bytes': 80000,
        'dns_query_times': 10
    },
    {
        'tcp_packets': 50,
        'dist_port_tcp': 2,
        'external_ips': 1,
        'volume_bytes': 200000,
        'udp_packets': 20,
        'tcp_urg_packet': 0,
        'source_app_packets': 100,
        'remote_app_packets': 90,
        'source_app_bytes': 50000,
        'remote_app_bytes': 40000,
        'dns_query_times': 5
    }
    # Add more dictionaries for additional APKs as needed
]

# Initialize an empty DataFrame to store results
results_df = pd.DataFrame()

# Assuming the Random Forest model was trained and saved previously
# Load your trained Random Forest model (replace this with your actual loading method)
rf_model = RandomForestClassifier()  # Replace with the actual loaded model

# Train a simple RandomForest model for demonstration purposes
# (In your case, you would load a pre-trained model)
# Example training (remove or replace with actual model loading)
X_train = pd.DataFrame(apk_features_list)
y_train = [0, 1]  # Example labels, replace with actual labels
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
rf_model.fit(X_train_scaled, y_train)

# Iterate over each APK's features
for i, apk_features in enumerate(apk_features_list):
    # Convert the features into a DataFrame with one row
    X_new = pd.DataFrame([apk_features])

    # Scale the new data using the same scaler used for the training data
    X_new_scaled = scaler.transform(X_new)

    # Predict using the trained Random Forest model
    predicted_label = rf_model.predict(X_new_scaled)

    # Determine prediction result
    if predicted_label == 0:
        prediction_result = "Benign"
    else:
        prediction_result = "Malware"

    # Add predicted label and result to the DataFrame
    X_new['predicted_label'] = predicted_label
    X_new['prediction_result'] = prediction_result

    # Append this APK's result to the overall results DataFrame
    results_df = pd.concat([results_df, X_new], ignore_index=True)

# Save results to CSV file
csv_file_path = '/home/hpc/Desktop/result.csv'  # Replace with your desired file path
results_df.to_csv(csv_file_path, index=False)

print(f"Predictions saved to: {csv_file_path}")